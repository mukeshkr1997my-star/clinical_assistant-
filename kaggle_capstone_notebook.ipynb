{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8eaf7dc",
   "metadata": {},
   "source": [
    "# Kaggle Capstone Notebook — Google Cloud (ADK/SDK) Integration\n",
    "\n",
    "This notebook is a template for a Kaggle Capstone project that integrates with Google Cloud services (GCS, BigQuery, Vertex AI, Secret Manager, Cloud SQL) using the Google Cloud SDK / client libraries ('ADK' shorthand). It is written to run in a Kaggle Notebook environment, where you can upload a **service account JSON** as a Kaggle Dataset (private) and mount it, or use Application Default Credentials locally.\n",
    "\n",
    "> ⚠️ Security reminder: Never commit service account keys to a public repo. In Kaggle, add the service account JSON as a **private dataset** and attach it to the kernel. Remove keys when done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8ceb",
   "metadata": {},
   "source": [
    "## Quick overview\n",
    "\n",
    "This notebook includes:\n",
    "- Authentication patterns for Kaggle and local Jupyter\n",
    "- Optional installs for Google Cloud client libraries and ML tooling\n",
    "- Examples: GCS upload/download, BigQuery load & query, Vertex AI embedding/LLM pattern (pseudo-code), Cloud SQL connection\n",
    "- RAG demo using toy data and a simple TF retriever (no external deps)\n",
    "- Instructions on how to adapt for real Kaggle datasets and Capstone submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e831eff",
   "metadata": {},
   "source": [
    "## 0) How to provide Google credentials in Kaggle\n",
    "\n",
    "1. Create a service account in GCP with minimal scopes (Storage Object Admin for GCS access, BigQuery Data Viewer for read-only, Secret Manager Accessor if needed, Vertex AI User if using Vertex).\n",
    "2. Download the JSON key and add it to a **private Kaggle dataset** (not the public dataset).\n",
    "3. In the Kaggle notebook: open the \"Add data\" panel and attach the private dataset. The key file will appear under `/kaggle/input/<dataset-name>/`.\n",
    "4. Set environment variable `GOOGLE_APPLICATION_CREDENTIALS` to that path in the notebook before using client libraries.\n",
    "\n",
    "Example path in Kaggle: `/kaggle/input/my-gcp-key/service-account.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fff30",
   "metadata": {},
   "source": [
    "## 1) Optional installs\n",
    "\n",
    "Uncomment to install additional libraries in the Kaggle runtime. Kaggle typically has many packages preinstalled; adjust as needed.\n",
    "\n",
    "```python\n",
    "# !pip install --quiet google-cloud-storage google-cloud-bigquery google-cloud-secret-manager google-cloud-aiplatform sqlalchemy psycopg2-binary\n",
    "# Optional for embeddings locally: sentence-transformers faiss-cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a963e",
   "metadata": {},
   "source": [
    "## 2) Set project variables and auth\n",
    "\n",
    "Edit these variables: `PROJECT_ID`, `BUCKET`, `REGION`. For Kaggle, point `SERVICE_ACCOUNT_PATH` to the uploaded private dataset file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT these for your GCP project\n",
    "PROJECT_ID = 'your-gcp-project'\n",
    "BUCKET = 'your-gcs-bucket'\n",
    "REGION = 'us-central1'\n",
    "# In Kaggle, upload your service account JSON as a private dataset and set the path below\n",
    "SERVICE_ACCOUNT_PATH = '/kaggle/input/my-gcp-key/service-account.json'  # update if different\n",
    "\n",
    "# Set credentials for Google client libs\n",
    "if os.path.exists(SERVICE_ACCOUNT_PATH):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = SERVICE_ACCOUNT_PATH\n",
    "    print('Using service account:', SERVICE_ACCOUNT_PATH)\n",
    "else:\n",
    "    print('Service account JSON not found at', SERVICE_ACCOUNT_PATH)\n",
    "    print('In local dev, use gcloud auth application-default login or set GOOGLE_APPLICATION_CREDENTIALS environment variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad090dd",
   "metadata": {},
   "source": [
    "## 3) Authenticate and basic GCS example\n",
    "\n",
    "This cell shows how to upload/download files to Google Cloud Storage. In Kaggle, ensure the service account has Storage permissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.cloud import storage\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(BUCKET)\n",
    "    print('GCS client ready for project', PROJECT_ID)\n",
    "except Exception as e:\n",
    "    print('GCS client not available or credentials missing:', e)\n",
    "\n",
    "# Example functions\n",
    "def upload_to_gcs(local_path, bucket_name, gcs_path):\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f'Uploaded {local_path} -> gs://{bucket_name}/{gcs_path}')\n",
    "\n",
    "def download_from_gcs(bucket_name, gcs_path, local_path):\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.download_to_filename(local_path)\n",
    "    print(f'Downloaded gs://{bucket_name}/{gcs_path} -> {local_path}')\n",
    "\n",
    "# Sanity check (list first 10 objects) - requires permission\n",
    "try:\n",
    "    blobs = list(bucket.list_blobs(max_results=10))\n",
    "    print('First 10 objects in bucket:')\n",
    "    for b in blobs:\n",
    "        print('-', b.name)\n",
    "except Exception as e:\n",
    "    print('Could not list bucket contents (check permissions):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a148a",
   "metadata": {},
   "source": [
    "## 4) BigQuery: load CSV and query (EMR / Capstone data)\n",
    "\n",
    "Use BigQuery for tabular EMR exports or large datasets. This example loads a local CSV into BigQuery and runs a simple query. Ensure the service account has BigQuery Data Editor or Owner role for load operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.cloud import bigquery\n",
    "    bq = bigquery.Client(project=PROJECT_ID)\n",
    "    print('BigQuery client ready')\n",
    "except Exception as e:\n",
    "    print('BigQuery client not available or credentials missing:', e)\n",
    "\n",
    "# Example: load CSV to BigQuery (uncomment and edit paths to run)\n",
    "# dataset_id = f\"{PROJECT_ID}.capstone_demo\"\n",
    "# table_id = f\"{dataset_id}.emr_records\"\n",
    "# bq.create_dataset(dataset_id, exists_ok=True)\n",
    "# job_config = bigquery.LoadJobConfig(autodetect=True, write_disposition='WRITE_TRUNCATE')\n",
    "# with open('/kaggle/input/my-emr/emr_export.csv','rb') as f:\n",
    "#     load_job = bq.load_table_from_file(f, table_id, job_config=job_config)\n",
    "# load_job.result()\n",
    "# print('Loaded EMR CSV to', table_id)\n",
    "\n",
    "# Example query (replace table_id)\n",
    "# sql = f'SELECT * FROM `{table_id}` LIMIT 10'\n",
    "# df = bq.query(sql).to_dataframe()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c578861",
   "metadata": {},
   "source": [
    "## 5) Cloud SQL: connect (optional)\n",
    "\n",
    "If you have an operational Cloud SQL instance, use the Cloud SQL Python connector or SQLAlchemy. For Kaggle, use private IP or configure an authorized network (be cautious). Alternatively, use exported CSVs or BigQuery for analysis in Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SQLAlchemy connection (do not hardcode credentials)\n",
    "# from sqlalchemy import create_engine\n",
    "# DB_USER = 'db_user'\n",
    "# DB_PASS = os.environ.get('CLOUD_SQL_DB_PASS')\n",
    "# DB_NAME = 'clinicaldb'\n",
    "# DB_HOST = '34.123.45.67:5432'\n",
    "# engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME}')\n",
    "# with engine.connect() as conn:\n",
    "#     print(conn.execute('SELECT 1').fetchall())\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882423a8",
   "metadata": {},
   "source": [
    "## 6) Vertex AI: embeddings & LLM pattern (safe)\n",
    "\n",
    "Kaggle notebooks can call Vertex AI endpoints. Prefer de-identified text and log requests. The Vertex SDK evolves; below is a pattern (pseudo-code) — replace with the appropriate `google-cloud-aiplatform` calls for your SDK version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af667bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO-CODE: Replace with your Vertex SDK calls\n",
    "# from google.cloud import aiplatform\n",
    "# aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "# def get_embeddings(texts, model='textembedding-gecko@001'):\n",
    "#     model = aiplatform.TextEmbeddingModel.from_pretrained(model)\n",
    "#     embs = model.get_embeddings(texts)\n",
    "#     return embs\n",
    "#\n",
    "# def call_vertex_llm(prompt, model='text-bison@001'):\n",
    "#     model = aiplatform.TextGenerationModel.from_pretrained(model)\n",
    "#     out = model.predict(prompt)\n",
    "#     return out.text\n",
    "print('Vertex AI pattern cell - implement with your SDK version')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4feb58",
   "metadata": {},
   "source": [
    "## 7) RAG demo: toy data + TF retriever (works offline in Kaggle)\n",
    "\n",
    "This is the same lightweight retriever used in the project: term-frequency vectors + cosine similarity. It runs without external libraries and demonstrates the end-to-end flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99781be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy docs\n",
    "toy_docs = [\n",
    "    {'id':'doc1','text':'68-year-old male with chest pain, troponin elevated, ST-elevation.'},\n",
    "    {'id':'doc2','text':'45F with fever, productive cough, right lower lobe consolidation - pneumonia.'},\n",
    "    {'id':'doc3','text':'70F with dyspnea, elevated BNP, reduced EF consistent with heart failure.'}\n",
    "]\n",
    "\n",
    "# Build TF index\n",
    "import re, math\n",
    "vocab = {}\n",
    "doc_freqs = {}\n",
    "for doc in toy_docs:\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", doc['text'].lower())\n",
    "    freqs = {}\n",
    "    for t in tokens:\n",
    "        freqs[t] = freqs.get(t,0)+1\n",
    "        if t not in vocab:\n",
    "            vocab[t] = len(vocab)\n",
    "    doc_freqs[doc['id']] = freqs\n",
    "vectors = {}\n",
    "for did, freqs in doc_freqs.items():\n",
    "    vec = [0]*len(vocab)\n",
    "    for term,count in freqs.items():\n",
    "        vec[vocab[term]] = count\n",
    "    vectors[did] = vec\n",
    "\n",
    "# Retriever\n",
    "def cosine(a,b):\n",
    "    dot = sum(x*y for x,y in zip(a,b))\n",
    "    na = math.sqrt(sum(x*x for x in a))\n",
    "    nb = math.sqrt(sum(x*x for x in b))\n",
    "    if na==0 or nb==0: return 0.0\n",
    "    return dot/(na*nb)\n",
    "\n",
    "def query_vec(q):\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", q.lower())\n",
    "    vec = [0]*len(vocab)\n",
    "    for t in tokens:\n",
    "        if t in vocab:\n",
    "            vec[vocab[t]] += 1\n",
    "    return vec\n",
    "\n",
    "def retrieve(query, top_k=2):\n",
    "    qv = query_vec(query)\n",
    "    scores = []\n",
    "    for did, dv in vectors.items():\n",
    "        # pad\n",
    "        if len(dv) < len(qv):\n",
    "            dv = dv + [0]*(len(qv)-len(dv))\n",
    "        elif len(dv) > len(qv):\n",
    "            qv = qv + [0]*(len(dv)-len(qv))\n",
    "        scores.append((did, cosine(qv,dv)))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n",
    "\n",
    "print('Retrieve example:', retrieve('chest pain troponin ST-elevation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57f6fa",
   "metadata": {},
   "source": [
    "## 8) Mock LLM + orchestrator (offline demo)\n",
    "\n",
    "A deterministic mock LLM concatenates evidence and emits a cautious summary. For production, replace with Vertex or other LLM calls and ensure de-identification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm(retrieved_docs):\n",
    "    out = ['EVIDENCE:']\n",
    "    for d in retrieved_docs:\n",
    "        out.append(f\"- {d['id']}: {d['text']}\")\n",
    "    summary = 'Summary: ' + ('; '.join([d['text'] for d in retrieved_docs]) )\n",
    "    out.append('\\n' + summary)\n",
    "    out.append('\\nRecommendation: Validate findings with primary record; consult specialists.')\n",
    "    return '\\n'.join(out)\n",
    "\n",
    "# Orchestrator\n",
    "def run_query(query):\n",
    "    hits = retrieve(query, top_k=2)\n",
    "    retrieved = []\n",
    "    for did, score in hits:\n",
    "        text = next(d['text'] for d in toy_docs if d['id']==did)\n",
    "        retrieved.append({'id':did,'score':score,'text':text})\n",
    "    return mock_llm(retrieved)\n",
    "\n",
    "print(run_query('fever cough consolidation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1b812",
   "metadata": {},
   "source": [
    "## 9) Kaggle Capstone tips\n",
    "- Use BigQuery for large datasets and exploratory SQL queries. Export features/tables to CSV for model training in Kaggle.\n",
    "- Mount GCS buckets for large artifact storage (models, indexes).\n",
    "- For reproducibility, save index files and model artifacts to GCS with versioned paths.\n",
    "- Use Kaggle's dataset publishing to share non-sensitive parts of your capstone (models, notebooks) and avoid PHI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c55f46",
   "metadata": {},
   "source": [
    "## 10) Safety, compliance & next steps\n",
    "- This notebook is a demo. **Do not** use outputs for autonomous clinical decisions.\n",
    "- De-identify PHI before sending data to external services.\n",
    "- Use Secret Manager and least-privilege service accounts.\n",
    "- For production: implement end-to-end tests, CI, audit logs (BigQuery), and governance reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c798e",
   "metadata": {},
   "source": [
    "## Files and artifacts\n",
    "This notebook assumes you'll attach a private Kaggle dataset containing your service account JSON (for GCP access). It also illustrates saving an index file to GCS if you choose to build embeddings/FAISS locally in Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aea708",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "If you'd like, I can now:\n",
    "- Save this notebook to `/mnt/data/kaggle_capstone_notebook.ipynb` for download (I will do that now).\n",
    "- Add cells that build an embedding-based FAISS index in Kaggle (requires internet & optional installs).\n",
    "- Create a companion `README.md` for Kaggle dataset publishing tips.\n",
    "\n",
    "Tell me which you prefer next."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
